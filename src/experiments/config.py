import inspect
import json
from pathlib import Path
from typing import Tuple

import torch
import wandb
from torch import nn
from torch.utils.data import DataLoader

from src.constants import DEVICE, WANDB_ENTITY
from src.data.datahandler import get_datahandler_class
from src.experiments.registry import import_files_from
from src.models.utils import get_model_class, get_model
from src.train.train import train, load_checkpoint
from src.train.utils import DEFAULT_TRAIN_CONFIG, get_optimizer

# For Registry to work correctly, we need to first import all implementations
# If you don't want to import everything, comment it out and only import the models / datahandlers you're using
import_files_from("models")
import_files_from("data")

PARAM_TO_FILL = "<FILL_ME>"


def run_config(config: dict, save_path: Path, experiment_name: str, run_name: str, log_wandb: bool = False, save_wandb: bool = True):
    print(f"Running experiment {experiment_name}, run {run_name}\nResults will be saved locally to {save_path}\n")

    wandb_run = None
    if log_wandb:
        wandb_run = wandb.init(entity=WANDB_ENTITY, reinit=True, config=config, project=experiment_name, name=run_name)

    parse_config_and_train(config, save_path, run_name, wandb_run, save_wandb)

    if wandb_run:
        wandb_run.finish()


def parse_config_and_train(config: dict, save_path: Path, run_name: str, wandb_run = None, save_wandb: bool = True):
    # extracts models / datasets, trains a model, and saves the results
    print(json.dumps(config, indent=4))

    model, optimizer = get_model_and_optimizer(config)
    train_dataloader, val_dataloader = get_dataloaders(config)

    train(config=config, model=model, optimizer=optimizer, train_dataloader=train_dataloader,
          val_dataloader=val_dataloader, save_path=save_path, save_name=run_name, wandb_run=wandb_run,
          save_wandb=save_wandb)


def get_model_and_optimizer(config: dict) -> Tuple[nn.Module, torch.optim.Optimizer]:
    # loads a pretrained model if possible, otherwise initializes a model by name from the config file
    pretrained_config_path = config["model"].get("from_pretrained")
    if pretrained_config_path:
        model_path = Path(pretrained_config_path).with_suffix('.pth')
        model, optimizer = load_checkpoint(Path(model_path))
        print(f"Loaded a pre-trained model from {model_path}")
        return model, optimizer

    model = get_model(config)
    optimizer = get_optimizer(config, model)
    return model, optimizer


def get_dataloaders(config: dict) -> Tuple[DataLoader, DataLoader]:
    # loads train/val dataloaders by name from the config file
    # Todo: make the datasets the same format?
    dataset_name = config["dataset"]["name"]
    dataset_params = config["dataset"]["params"]
    datahander = get_datahandler_class(dataset_name)(**dataset_params)
    return datahander.get_train_val_dataloaders()


def generate_config(model: str, dataset: str, var_name: str = "cur_config", print_config: bool = True) -> dict:
    # A helper function to generate a config for the given model and datahandler, with some parameters to be filled in.
    # Just copy the output, paste it wherever and fill the parameters, it should be enough to successfully load
    # the model and the datahandler with the given parameters.
    # The training config's structure is [DEFAULT_TRAIN_CONFIG]
    config = {
        "model": {
            "name": model,
            "params": get_params(get_model_class(model))
        },
        "dataset": {
            "name": dataset,
            "params": get_params(get_datahandler_class(dataset))
        },
        "train": DEFAULT_TRAIN_CONFIG
    }
    comment = "# Config generated by src.experiments.config.py#generate_config"
    init_var_code = (f"{comment}\n"
                     f"{var_name} = {format_value(config)}")
    if print_config:
        print(init_var_code)
    return config


def generate_finetuning_config(pretrained_config_path: Path, dataset: str, var_name: str = "cur_config") -> dict:
    # A helper function to generate a config to fine tune a pretrained model on a given dataset
    # It copies a 'model' config of the pretrained model for configs compatibility
    # Note: pass a config of the PRETRAINED model

    model_path = pretrained_config_path.with_suffix('.pth')
    assert model_path.exists(), f"No model found for config {pretrained_config_path.absolute()}"

    pretrained_config = json.loads(pretrained_config_path.read_bytes())
    config = generate_config(pretrained_config['model']['name'], dataset, print_config=False)
    config['model'] = pretrained_config['model']
    config['model']['from_pretrained'] = str(pretrained_config_path.absolute())

    comment = ("# Config generated by src.experiments.config.py#generate_finetuning_config\n"
               "# DON'T CHANGE THE MODELS PARAMS TO SUCCESSFULLY LOAD THE PRETRAINED MODEL")
    init_var_code = (f"{comment}\n"
                     f"{var_name} = {format_value(config)}")
    print(init_var_code)
    return config


def get_params(init_fn) -> dict:
    # Extracts parameters from the __init__ method with default values if present.
    # Used to provide all the necessary parameters for model/datahandler classes in configs
    params = inspect.signature(init_fn).parameters

    def fill_empy(default_value):
        return default_value if default_value is not inspect._empty else PARAM_TO_FILL

    return {name: fill_empy(param.default) for name, param in params.items() if name != "self"}


def format_value(value, indent_level=0, indent=4, indent_char=' '):
    # To pretty print the config dictionary
    left_indent = indent_char * indent_level
    if isinstance(value, dict):
        items = []
        for k, v in value.items():
            formatted_key = repr(k)
            formatted_value = format_value(v, indent_level=indent_level + indent)
            todo_prefix = "#TODO " if v == PARAM_TO_FILL else ""
            items.append(f"{left_indent}{indent_char * indent}{todo_prefix}{formatted_key}: {formatted_value}")
        return "{\n" + ",\n".join(items) + f"\n{left_indent}}}"
    elif isinstance(value, str) and value == PARAM_TO_FILL:
        return value
    else:
        return repr(value)


# Run to generate a template config
if __name__ == "__main__":
    generate_config("small_unet", "cil")
