# If called by wandb.agent, as below,
# this config will be set by Sweep Controller


from pathlib import Path

from src.experiments.config import run_config
from src.experiments.sweep_config import get_sweep_config
from src.experiments.utils import get_run_name, get_save_path_and_experiment_name

# Config generated by src.experiments.config.py#generate_config
cur_config = {
    'model': {
        'name': 'small_unet',
        'params': {
            'chs': (3, 64, 128, 256, 512, 1024)
        }
    },
    'dataset': {
        'name': 'cil',
        'params': {
            'batch_size': 4,
            'shuffle': True,
            'resize_to': (384, 384)
        }
    },
    'train': {
        'n_epochs': 1,
        'optimizer': {
            'name': 'Adam',
            'params': {
                'lr': [0.0005, 0.01, 0.001] # a list of possible options for sweep
            }
        },
        'loss': {
            'name': 'BCELoss',
            'params': { }
        },
        'clip_grad': [None, 1] # another list of possible options for sweep
    }
}


if __name__ == '__main__':
    save_path, experiment_name = get_save_path_and_experiment_name(__file__)
    run_name = get_run_name(cur_config)
    sweep_config = get_sweep_config(cur_config)
    print(sweep_config)

    # run_sweep_config(sweep_config)
    #
    # run_config(cur_config, save_path, experiment_name, run_name, log_wandb=True)

#
# with wandb.init(config=config, project=SWEEP_PROJECT, settings=wandb.Settings(start_method='spawn')) as run:
#     w_config = wandb.config
#     config = get_full_config(
#         d_model=w_config.d_model,
#         weight_decay=w_config.weight_decay,
#         epochs=w_config.epochs,
#         clip=w_config.clip,
#         lr=w_config.lr,
#         lr_lambda=w_config.lr_lambda,
#         betas=w_config.betas
#     )
#
#     print(json.dumps(config, indent=4))
